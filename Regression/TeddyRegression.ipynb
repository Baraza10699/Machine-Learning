{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression on Bank Notes Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data:\n",
    "\n",
    "Data was extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Temp/ipykernel_11880/3878583966.py:1: DeprecationWarning: The symbol module is deprecated and will be removed in future versions of Python\n",
      "  from symbol import import_as_name\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symbol import import_as_name\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"C:/Users/teddy/Downloads/Machine Learning/Perceptron HW/BankNote_Authentication.csv\")\n",
    "\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Temp/ipykernel_16112/435437580.py:20: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  sc_y.fit(y_train[:, np.newaxis])\n",
      "C:\\Users\\teddy\\AppData\\Local\\Temp/ipykernel_16112/435437580.py:21: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  y_train_std = sc_y.transform(y_train[:, np.newaxis]).flatten()\n",
      "C:\\Users\\teddy\\AppData\\Local\\Temp/ipykernel_16112/435437580.py:22: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  y_test_std = sc_y.transform(y_test[:, np.newaxis]).flatten()\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = dataset.iloc[:, [0, 2]]\n",
    "y = dataset.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "sc_y.fit(y_train[:, np.newaxis])\n",
    "y_train_std = sc_y.transform(y_train[:, np.newaxis]).flatten()\n",
    "y_test_std = sc_y.transform(y_test[:, np.newaxis]).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training sklearn's LinearRegression on the Bank Notes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.438, test: 0.488\n",
      "R^2 train: 0.562, test: 0.508\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "est = LinearRegression()  \n",
    "est.fit(X_train_std, y_train_std)\n",
    "\n",
    "\n",
    "y_train_pred = est.predict(X_train_std)\n",
    "y_test_pred = est.predict(X_test_std)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train_std, y_train_pred),\n",
    "        mean_squared_error(y_test_std, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train_std, y_train_pred),\n",
    "        r2_score(y_test_std, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training gplearn on the Bank Notes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    17.07          6.61563       32         0.537025              N/A     54.17s\n",
      "   1    13.61          1.60768        6         0.505146              N/A     55.26s\n",
      "   2    14.37          148.285        3         0.481675              N/A     52.84s\n",
      "   3    16.16          14.5099        4         0.478381              N/A     59.27s\n",
      "   4    17.24          1.06745        9         0.434406              N/A     54.97s\n",
      "   5    18.13          1.58507       19         0.418669              N/A     50.97s\n",
      "   6    19.19          1.18485       35         0.431977              N/A      1.21m\n",
      "   7    20.65          1.41678       27         0.415703              N/A      1.31m\n",
      "   8    19.68          2.05735        8         0.405458              N/A      1.36m\n",
      "   9    19.51          1.12331       75         0.378002              N/A      1.23m\n",
      "  10    21.09         0.981202       17         0.377271              N/A      1.25m\n",
      "  11    27.48         0.829319       41         0.352188              N/A      1.25m\n",
      "  12    33.12         0.836067       19         0.345632              N/A      1.26m\n",
      "  13    40.45         0.736109       31         0.337438              N/A      1.29m\n",
      "  14    45.08         0.747499       10         0.334236              N/A      1.27m\n",
      "  15    47.05          1.01547       34          0.32261              N/A      1.37m\n",
      "  16    43.76          0.92208       34         0.327169              N/A      1.93m\n",
      "  17    41.71         0.805551       43         0.319257              N/A      1.48m\n",
      "  18    43.30         0.779046       35         0.319257              N/A      1.30m\n",
      "  19    44.03          17.0679       44         0.319257              N/A      1.39m\n",
      "  20    41.92         0.698659      103         0.315229              N/A      2.22m\n",
      "  21    41.83         0.826713      103         0.315229              N/A      1.72m\n",
      "  22    43.51         0.874935       73         0.315229              N/A      1.42m\n",
      "  23    44.54         0.670913       94         0.315229              N/A      1.90m\n",
      "  24    47.68         0.742669       61         0.314572              N/A      1.39m\n",
      "  25    50.49         0.757847       54         0.304573              N/A      1.36m\n",
      "  26    52.68         0.817663       64         0.304573              N/A      1.33m\n",
      "  27    55.76         0.688195       59         0.304573              N/A      1.45m\n",
      "  28    58.23          0.71583       41         0.304573              N/A      1.31m\n",
      "  29    60.13         0.642267       68         0.304197              N/A      1.28m\n",
      "  30    59.24         0.600091       62         0.303705              N/A      1.31m\n",
      "  31    57.93         0.635858       81          0.28196              N/A      1.18m\n",
      "  32    57.46         0.724849       74         0.280616              N/A      1.31m\n",
      "  33    56.71         0.666707      110         0.280616              N/A      1.23m\n",
      "  34    57.04          1.93014      161         0.280616              N/A      1.29m\n",
      "  35    55.27         0.769304       73         0.236422              N/A      1.13m\n",
      "  36    56.74          3.16488       73         0.236422              N/A      1.15m\n",
      "  37    63.52         0.788351       74         0.236422              N/A      1.18m\n",
      "  38    74.03          7.20437       65         0.236422              N/A      1.23m\n",
      "  39    85.66         0.956556       65         0.230801              N/A      1.33m\n",
      "  40    89.09         0.838714      109         0.227371              N/A      1.43m\n",
      "  41    90.23         0.786411      107         0.227371              N/A      1.31m\n",
      "  42    89.28         0.799671      124         0.227371              N/A      1.31m\n",
      "  43    90.91         0.904714      109         0.219747              N/A      1.24m\n",
      "  44    92.65          86.0504      119         0.216568              N/A      1.39m\n",
      "  45    95.42         0.871525      123         0.216568              N/A      1.28m\n",
      "  46    99.79         0.814789      117         0.217612              N/A      1.30m\n",
      "  47   102.05         0.823545      108         0.215291              N/A      1.34m\n",
      "  48   105.30         0.681799       94         0.217612              N/A      1.25m\n",
      "  49   103.69         0.703801      111         0.215291              N/A      1.27m\n",
      "  50    99.55         0.830652      108         0.215291              N/A      1.19m\n",
      "  51    98.39         0.741331      113         0.215291              N/A      1.26m\n",
      "  52    96.19         0.875389      115         0.215291              N/A      1.15m\n",
      "  53    93.97         0.705279      121         0.215291              N/A      1.19m\n",
      "  54    94.17         0.756025       70         0.215209              N/A      1.11m\n",
      "  55    96.52          1.18847       55         0.215209              N/A      1.15m\n",
      "  56    97.65         0.796656       73         0.210901              N/A      1.09m\n",
      "  57   101.80         0.693245       87         0.210901              N/A      1.08m\n",
      "  58    98.91         0.684755       57         0.210901              N/A      1.00m\n",
      "  59    98.73         0.834511       57         0.210901              N/A     57.58s\n",
      "  60    97.63         0.768113      194         0.213079              N/A     56.31s\n",
      "  61    94.88         0.963904      122         0.213865              N/A     58.66s\n",
      "  62    93.52         0.793116       56         0.210901              N/A     54.26s\n",
      "  63    93.12         0.781624       56         0.210901              N/A     50.43s\n",
      "  64    92.36         0.693478       87         0.210454              N/A     56.35s\n",
      "  65    90.86         0.759052       61         0.210483              N/A      1.12m\n",
      "  66    91.97         0.804597      112         0.209778              N/A     51.86s\n",
      "  67    90.46         0.709695      112         0.209778              N/A     48.43s\n",
      "  68    89.91         0.739086       60         0.210483              N/A     45.26s\n",
      "  69    90.91         0.824511      148         0.205395              N/A     41.89s\n",
      "  70    90.75         0.736874       68         0.205395              N/A     41.14s\n",
      "  71    87.38         0.693836      145         0.205395              N/A     39.14s\n",
      "  72    87.14         0.926779      141         0.205395              N/A     41.63s\n",
      "  73    86.68         0.716649       73         0.205395              N/A     38.16s\n",
      "  74    86.30         0.787138      135         0.205395              N/A     38.45s\n",
      "  75    84.98         0.740376      123         0.205395              N/A     32.13s\n",
      "  76    84.85         0.715908      121         0.205395              N/A     31.69s\n",
      "  77    87.66         0.800341      154         0.205395              N/A     32.52s\n",
      "  78    93.51           1.0756       90         0.203881              N/A     30.80s\n",
      "  79    96.75         0.891052      165         0.205395              N/A     28.24s\n",
      "  80   100.69         0.823074       80         0.203881              N/A     28.56s\n",
      "  81   101.72         0.832151      111         0.202954              N/A     27.35s\n",
      "  82   101.84         0.815908      111         0.202954              N/A     26.88s\n",
      "  83    99.90         0.978975       65         0.198699              N/A     25.00s\n",
      "  84    98.07         0.966589       65         0.198699              N/A     25.46s\n",
      "  85    95.63         0.919115       76         0.197843              N/A     20.79s\n",
      "  86    89.88          1.05373       77         0.197843              N/A     18.51s\n",
      "  87    87.81         0.981065       87         0.192315              N/A     17.85s\n",
      "  88    88.04          1.37042       79         0.197843              N/A     16.61s\n",
      "  89    91.16         0.957639       78         0.186583              N/A     15.31s\n",
      "  90    90.89         0.786161       96         0.186583              N/A     13.31s\n",
      "  91    89.95         0.958602      105         0.186583              N/A     11.94s\n",
      "  92    89.50         0.905036      105         0.186547              N/A      9.87s\n",
      "  93    90.50         0.777641      105         0.186547              N/A      8.58s\n",
      "  94    93.77          1.00142      105         0.186547              N/A      8.44s\n",
      "  95    95.53         0.814447      126         0.186583              N/A      6.46s\n",
      "  96    94.65         0.878281      206         0.186583              N/A      4.76s\n",
      "  97    98.05         0.665257      130         0.186583              N/A      3.15s\n",
      "  98   101.78         0.820586       93          0.18658              N/A      1.62s\n",
      "  99   108.61         0.738042       94          0.18658              N/A      0.00s\n",
      "MSE train: 0.330, test: 0.475\n",
      "R^2 train: 0.670, test: 0.521\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "est = SymbolicRegressor(population_size=1000,\n",
    "                        init_depth=(4,6),\n",
    "                        generations=100, stopping_criteria=0.01,\n",
    "                        p_crossover=0.3, p_subtree_mutation=0.35,\n",
    "                        p_hoist_mutation=0.0, p_point_mutation=0.35,\n",
    "                        max_samples=1.0, verbose=1,\n",
    "                        #const_range=None,\n",
    "                        const_range=(-1.0,1.0),\n",
    "                        tournament_size=5,\n",
    "                        function_set=('add', 'sub', 'mul', 'div', 'sqrt', 'log', \n",
    "                                      'abs', 'neg', 'inv', 'max','min', 'sin', 'cos', 'tan'),\n",
    "                        parsimony_coefficient=0.0001, random_state=0)\n",
    "est.fit(X_train_std, y_train_std)\n",
    "\n",
    "y_train_pred = est.predict(X_train_std)\n",
    "y_test_pred = est.predict(X_test_std)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train_std, y_train_pred),\n",
    "        mean_squared_error(y_test_std, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train_std, y_train_pred),\n",
    "        r2_score(y_test_std, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training gplearn on the artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    17.38          22.1151       10         0.443625              N/A      1.13m\n",
      "   1    13.62          6.92078       18         0.250898              N/A      1.40m\n",
      "   2    13.35          1.45073       18         0.250898              N/A      1.29m\n",
      "   3    14.03          1.51288       19         0.244336              N/A      1.11m\n",
      "   4    13.64          1.37187       22         0.216357              N/A      1.15m\n",
      "   5    13.82          1.61913       34         0.222307              N/A      1.11m\n",
      "   6    17.93          1.23661       18         0.222307              N/A      1.27m\n",
      "   7    22.65         0.816092       23         0.185269              N/A      1.70m\n",
      "   8    24.06         0.858898       17         0.176855              N/A      1.33m\n",
      "   9    25.00         0.508156       23         0.177503              N/A      1.21m\n",
      "  10    27.04          0.56434       23         0.177503              N/A      1.30m\n",
      "  11    29.21         0.624548       17         0.164484              N/A      1.31m\n",
      "  12    31.07         0.437484       17         0.164484              N/A      1.27m\n",
      "  13    31.51         0.446984       28         0.160748              N/A      1.47m\n",
      "  14    33.17         0.543669       17         0.164484              N/A      1.35m\n",
      "  15    35.91         0.471116       70         0.161096              N/A      1.31m\n",
      "  16    36.61         0.556988       58         0.162984              N/A      1.44m\n",
      "  17    38.91         0.458505       74         0.141005              N/A      1.68m\n",
      "  18    39.67         0.582678       74         0.139061              N/A      1.43m\n",
      "  19    40.09         0.437208       74         0.135979              N/A      1.28m\n",
      "  20    42.19         0.392612       54         0.108206              N/A      1.27m\n",
      "  21    44.91         0.334758       45         0.108256              N/A      1.22m\n",
      "  22    50.55         0.311434       45         0.108226              N/A      1.33m\n",
      "  23    56.43         0.498264       45          0.10824              N/A      1.53m\n",
      "  24    63.75         0.316539       46         0.107511              N/A      1.96m\n",
      "  25    70.10         0.272349       54         0.106085              N/A      1.58m\n",
      "  26    69.19         0.251629       56         0.105748              N/A      1.56m\n",
      "  27    71.35         0.238124      108         0.104471              N/A      1.75m\n",
      "  28    72.23         0.253307       54         0.101074              N/A      1.60m\n",
      "  29    67.17         0.269057       54         0.101074              N/A      1.64m\n",
      "  30    67.90          0.22523       54         0.101074              N/A      1.30m\n",
      "  31    65.67         0.248943       97         0.100682              N/A      1.35m\n",
      "  32    67.14         0.221596       60         0.099249              N/A      1.31m\n",
      "  33    66.15         0.257967       96        0.0974557              N/A      1.40m\n",
      "  34    66.48          0.24286       96        0.0967306              N/A      1.22m\n",
      "  35    67.29         0.254622       96        0.0957397              N/A      1.01m\n",
      "  36    69.40         0.259336      102        0.0911403              N/A     59.30s\n",
      "  37    69.44         0.252999      166        0.0910798              N/A      1.04m\n",
      "  38    74.58         0.252811      123        0.0909474              N/A      1.01m\n",
      "  39    76.28         0.254481      118        0.0908488              N/A      1.10m\n",
      "  40    76.90         0.232863       88        0.0907127              N/A     59.46s\n",
      "  41    75.48         0.341186       88        0.0906963              N/A      1.08m\n",
      "  42    73.53         0.237537      193        0.0906963              N/A     56.80s\n",
      "  43    68.26         0.230483      101        0.0906833              N/A     51.45s\n",
      "  44    67.99         0.257772       72        0.0881547              N/A     51.78s\n",
      "  45    67.05         0.267925       84        0.0881475              N/A     53.53s\n",
      "  46    67.94         0.235886       84        0.0881803              N/A     47.32s\n",
      "  47    66.26         0.255817       70        0.0897843              N/A     48.57s\n",
      "  48    66.89         0.256882      127        0.0881235              N/A     47.93s\n",
      "  49    67.54         0.280745      199        0.0881235              N/A     55.76s\n",
      "  50    66.74         0.275334      194        0.0881547              N/A     44.65s\n",
      "  51    63.33         0.317826      194        0.0871201              N/A     49.97s\n",
      "  52    62.80         0.270703      202        0.0871201              N/A     40.84s\n",
      "  53    62.31         0.267026       92        0.0886971              N/A     42.27s\n",
      "  54    60.60          0.26767       84        0.0885706              N/A     39.34s\n",
      "  55    61.58         0.260917       95        0.0885632              N/A     39.01s\n",
      "  56    60.16         0.262358      117        0.0885615              N/A     40.21s\n",
      "  57    62.78         0.261081       77        0.0884802              N/A     38.92s\n",
      "  58    63.63           0.2984       83        0.0883232              N/A     41.03s\n",
      "  59    66.03         0.226534       83        0.0883232              N/A     38.50s\n",
      "  60    64.67         0.238786       88        0.0882815              N/A     36.41s\n",
      "  61    65.81         0.807449       86        0.0871236              N/A     39.96s\n",
      "  62    65.36         0.249068       95        0.0867921              N/A     35.61s\n",
      "  63    65.90          0.26592       68        0.0877748              N/A     35.24s\n",
      "  64    65.17          0.26378       68        0.0877748              N/A     32.41s\n",
      "  65    63.40         0.264507       66        0.0871709              N/A     30.47s\n",
      "  66    62.09          0.31361       78        0.0864168              N/A     31.17s\n",
      "  67    63.95         0.269499       78        0.0864168              N/A     28.68s\n",
      "  68    64.48         0.296165       83        0.0864576              N/A     28.30s\n",
      "  69    63.90         0.264411       67        0.0880923              N/A     32.27s\n",
      "  70    64.28         0.256227       37        0.0864746              N/A     29.94s\n",
      "  71    63.64         0.294316       94        0.0879024              N/A     29.96s\n",
      "  72    63.89         0.241756       92        0.0873561              N/A     28.57s\n",
      "  73    64.51         0.256279       70        0.0871565              N/A     23.16s\n",
      "  74    65.27          0.28724      102         0.087136              N/A     28.27s\n",
      "  75    65.98         0.245621      103        0.0870486              N/A     27.11s\n",
      "  76    64.70         0.258103       60        0.0856421              N/A     26.44s\n",
      "  77    65.94         0.274781       78        0.0860618              N/A     26.69s\n",
      "  78    67.34         0.259545       60        0.0850547              N/A     18.88s\n",
      "  79    66.28         0.266298       78        0.0861883              N/A     17.65s\n",
      "  80    65.93         0.240828       66         0.085935              N/A     18.89s\n",
      "  81    64.67         0.293802      101        0.0853311              N/A     16.20s\n",
      "  82    64.61         0.292674      110        0.0844259              N/A     17.05s\n",
      "  83    65.16         0.275814      108        0.0847834              N/A     15.27s\n",
      "  84    67.20         0.253466       55        0.0837621              N/A     14.83s\n",
      "  85    69.60         0.243675       63        0.0837249              N/A     15.66s\n",
      "  86    68.61         0.288351       65        0.0825138              N/A     15.09s\n",
      "  87    66.81         0.280117       66        0.0825119              N/A     13.97s\n",
      "  88    66.63          0.29779       77        0.0816428              N/A     12.14s\n",
      "  89    64.54          0.86241      111        0.0814095              N/A     11.28s\n",
      "  90    67.06          0.26672      115        0.0814095              N/A      9.55s\n",
      "  91    65.24         0.317502       57        0.0814095              N/A      9.14s\n",
      "  92    65.17         0.296244       80        0.0801944              N/A      7.79s\n",
      "  93    64.57         0.305818       91        0.0807098              N/A      6.60s\n",
      "  94    65.65         0.274307       68        0.0814095              N/A      5.70s\n",
      "  95    64.50          0.30588       79        0.0808444              N/A      4.88s\n",
      "  96    66.03         0.289296       91        0.0789793              N/A      2.67s\n",
      "  97    66.81         0.288593       50        0.0789793              N/A      1.80s\n",
      "  98    66.44         0.319249      102        0.0789793              N/A      0.91s\n",
      "  99    66.70         0.318231      106        0.0789793              N/A      0.00s\n",
      "MSE train: 0.079, test: 0.113\n",
      "R^2 train: 0.681, test: 0.541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "##################\n",
    "\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "est = SymbolicRegressor(population_size=1000,\n",
    "                        init_depth=(4,6),\n",
    "                        generations=100, stopping_criteria=0.01,\n",
    "                        p_crossover=0.3, p_subtree_mutation=0.35,\n",
    "                        p_hoist_mutation=0.0, p_point_mutation=0.35,\n",
    "                        max_samples=1.0, verbose=1,\n",
    "                        const_range=None,\n",
    "                        #const_range=(-1.0,1.0),\n",
    "                        tournament_size=5,\n",
    "                        function_set=('add', 'sub', 'mul', 'div', 'sqrt', 'log', \n",
    "                                      'abs', 'neg', 'inv', 'max','min', 'sin', 'cos', 'tan'),\n",
    "                        parsimony_coefficient=0.0001, random_state=0)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "##################\n",
    "\n",
    "y_train_pred = est.predict(X_train)\n",
    "y_test_pred = est.predict(X_test)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis\n",
    "\n",
    "The gplearn has a higher R^2 model score than LinearRegression's showing that gplearn fits the data beter.However gplearn has a higher difference with it's test R^2 score showinng that it slightly overfits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "623d2b1449816232213dd67ca4b2decd46844005dd9b91857ea828ab7f507aea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
